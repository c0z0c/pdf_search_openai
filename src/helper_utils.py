"""
Helper utilities for Jupyter Notebook environments.

This module provides various utility functions for:
- AI Hub dataset download and management
- File operations (zip/unzip with progress bars)
- Model saving/loading
- Directory tree visualization
- Logging configuration
"""

import json
import logging
import os
import re
import shutil
import sys
import tarfile
import unicodedata
import zipfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pytz
import requests
import torch
from tqdm.notebook import tqdm

try:
    # Colab í™˜ê²½ ì—¬ë¶€ í™•ì¸
    import google.colab
    IS_COLAB: bool = True
except ImportError:
    IS_COLAB: bool = False


__version__: str = "2.6.0"


class ShortLevelFormatter(logging.Formatter):
    """
    Custom logging formatter that abbreviates log levels to single characters.

    This formatter converts log level names to single-character abbreviations:
    DEBUGâ†’D, INFOâ†’I, WARNINGâ†’W, ERRORâ†’E, CRITICALâ†’C

    It also formats timestamps in Korean Standard Time (KST/Asia/Seoul).

    Attributes:
        LEVEL_MAP (Dict[str, str]): Mapping of full level names to abbreviations.
        kst (pytz.timezone): Korean Standard Time timezone object.
    """

    LEVEL_MAP: Dict[str, str] = {
        'DEBUG': 'D',
        'INFO': 'I',
        'WARNING': 'W',
        'ERROR': 'E',
        'CRITICAL': 'C'
    }
    kst: pytz.tzinfo.BaseTzInfo = pytz.timezone('Asia/Seoul')

    def format(self, record: logging.LogRecord) -> str:
        """
        Format the log record with abbreviated level name.

        Args:
            record (logging.LogRecord): The log record to format.

        Returns:
            str: Formatted log message string.
        """
        record.levelname = self.LEVEL_MAP.get(record.levelname, record.levelname)
        return super().format(record)

    def formatTime(
        self,
        record: logging.LogRecord,
        datefmt: Optional[str] = None
    ) -> str:
        """
        Convert record.created timestamp to KST and return formatted string.

        Args:
            record (logging.LogRecord): The log record containing timestamp.
            datefmt (Optional[str]): Custom date format string.

        Returns:
            str: Formatted timestamp in KST.
        """
        ct = datetime.fromtimestamp(record.created, tz=self.kst)
        if datefmt:
            return ct.strftime(datefmt)
        return ct.strftime('%Y-%m-%d %H:%M:%S')
    
if IS_COLAB:
    # Colab: ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ì¬ì„¤ì •
    logger = logging.getLogger()
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    handler = logging.StreamHandler()
    handler.setFormatter(ShortLevelFormatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    logger.addHandler(handler)
else:
    logging.basicConfig(
        format='%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    logger = logging.getLogger()
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ì˜ Formatter êµì²´
    for handler in logging.getLogger().handlers:
        handler.setFormatter(ShortLevelFormatter(
            '%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        ))

# logger.setLevel(logging.DEBUG)


class AIHubShell:
    """
    AI Hub API wrapper for dataset download and management.

    This class provides convenient methods to interact with the AI Hub API,
    including searching datasets, viewing file structures, and downloading datasets.

    Attributes:
        BASE_URL (str): Base URL for AI Hub API.
        LOGIN_URL (str): API key validation endpoint.
        BASE_DOWNLOAD_URL (str): Download endpoint base URL.
        MANUAL_URL (str): API manual documentation URL.
        BASE_FILETREE_URL (str): File tree structure endpoint base URL.
        DATASET_URL (str): Dataset information endpoint.
        DEBUG (bool): Enable debug output if True.
        download_dir (str): Directory path for downloaded files.
    """

    def __init__(self, DEBUG: bool = False, download_dir: Optional[str] = None) -> None:
        """
        Initialize AIHubShell instance.

        Args:
            DEBUG (bool, optional): Enable debug output. Defaults to False.
            download_dir (Optional[str], optional): Download directory path.
                Defaults to current directory (".").
        """
        self.BASE_URL: str = "https://api.aihub.or.kr"
        self.LOGIN_URL: str = f"{self.BASE_URL}/api/keyValidate.do"
        self.BASE_DOWNLOAD_URL: str = f"{self.BASE_URL}/down/0.5"
        self.MANUAL_URL: str = f"{self.BASE_URL}/info/api.do"
        self.BASE_FILETREE_URL: str = f"{self.BASE_URL}/info"
        self.DATASET_URL: str = f"{self.BASE_URL}/info/dataset.do"
        self.DEBUG: bool = DEBUG
        self.download_dir: str = download_dir if download_dir else "."

    def help(self) -> None:
        """
        Print usage guide for AIHubShell class.

        Displays comprehensive information about available methods,
        parameters, usage examples, and precautions.
        """
        print("=" * 80)
        print("                        AIHubShell í´ë˜ìŠ¤ ì‚¬ìš© ê°€ì´ë“œ")
        print("=" * 80)
        print()
        
        print("ğŸ”§ ì´ˆê¸°í™”")
        print("  AIHubShell(DEBUG=False, download_dir=None)")
        print("    DEBUG: Trueë¡œ ì„¤ì •í•˜ë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
        print("    download_dir: ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì§€ì • (ê¸°ë³¸ê°’: í˜„ì¬ ê²½ë¡œ)")
        print()
        
        print("ğŸ“‹ ë°ì´í„°ì…‹ ì¡°íšŒ")
        print("  .dataset_info()                    # ì „ì²´ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ")
        print("  .dataset_search('ê²€ìƒ‰ì–´')          # íŠ¹ì • ì´ë¦„ í¬í•¨ ë°ì´í„°ì…‹ ê²€ìƒ‰")
        print("  .dataset_search('ê²€ìƒ‰ì–´', tree=True) # ê²€ìƒ‰ + íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ")
        print("  .list_info(datasetkey=576)         # íŠ¹ì • ë°ì´í„°ì…‹ì˜ íŒŒì¼ ëª©ë¡")
        print("  .json_info(datasetkey=576)         # JSON í˜•íƒœë¡œ íŒŒì¼ êµ¬ì¡° ë°˜í™˜")
        print()
        
        print("ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
        print("  .download_dataset(apikey, datasetkey, filekeys='all')")
        print("    apikey: AI Hub API í‚¤")
        print("    datasetkey: ë°ì´í„°ì…‹ ë²ˆí˜¸")
        print("    filekeys: íŒŒì¼í‚¤ ('all' ë˜ëŠ” '66065,66083' í˜•íƒœ)")
        print("    overwrite: ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ì—¬ë¶€ (ê¸°ë³¸ê°’: False)")
        print()
        
        print("ğŸ“– ê¸°íƒ€ ê¸°ëŠ¥")
        print("  .print_usage()                     # AI Hub API ìƒì„¸ ì‚¬ìš©ë²•")
        print("  .help()                            # ì´ ë„ì›€ë§")
        print()
        
        print("ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ")
        print("  # 1. ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        print("  aihub = AIHubShell(DEBUG=True, download_dir='./data')")
        print()
        print("  # 2. ê²½êµ¬ì•½ì œ ë°ì´í„°ì…‹ ê²€ìƒ‰")
        print("  aihub.dataset_search('ê²½êµ¬ì•½ì œ')")
        print()
        print("  # 3. ë°ì´í„°ì…‹ 576ì˜ íŒŒì¼ ëª©ë¡ í™•ì¸")
        print("  aihub.list_info(datasetkey=576)")
        print()
        print("  # 4. íŠ¹ì • íŒŒì¼ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ")
        print("  aihub.download_dataset(")
        print("      apikey='YOUR_API_KEY',")
        print("      datasetkey=576,")
        print("      filekeys='66065,66083'")
        print("  )")
        print()
        print("  # 5. ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print("  aihub.download_dataset(")
        print("      apikey='YOUR_API_KEY',")
        print("      datasetkey=576,")
        print("      filekeys='all'")
        print("  )")
        print()
        
        print("âš ï¸  ì£¼ì˜ì‚¬í•­")
        print("  - API í‚¤ëŠ” AI Hubì—ì„œ ë°œê¸‰ë°›ì•„ì•¼ í•©ë‹ˆë‹¤")
        print("  - ëŒ€ìš©ëŸ‰ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œ ì¶©ë¶„í•œ ì €ì¥ ê³µê°„ì„ í™•ë³´í•˜ì„¸ìš”")
        print("  - overwrite=Falseì¼ ë•Œ ê¸°ì¡´ íŒŒì¼ì€ ìë™ìœ¼ë¡œ ê±´ë„ˆëœë‹ˆë‹¤")
        print("  - ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ë‹¤ìš´ë¡œë“œ ì‹œê°„ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print()
        
        print("ğŸ” ì¶”ê°€ ì •ë³´")
        print("  AI Hub API ê³µì‹ ë¬¸ì„œ: https://aihub.or.kr")
        print("  ë¬¸ì œ ë°œìƒ ì‹œ DEBUG=Trueë¡œ ì„¤ì •í•˜ì—¬ ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        print("=" * 80)

    def print_usage(self) -> None:
        """
        Print AI Hub API usage documentation.

        Fetches and displays the API manual from AI Hub, including
        command descriptions in English and Korean.

        Raises:
            requests.RequestException: If API request fails.
        """
        try:
            response = requests.get(self.MANUAL_URL)
            manual = response.text

            if self.DEBUG:
                print("API ì›ë³¸ ì‘ë‹µ:")
                print(manual)

            # JSON íŒŒì‹±í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            try:
                manual = re.sub(r'("FRST_RGST_PNTTM":)([0-9\- :\.]+)', r'\1"\2"', manual)
                manual_data = json.loads(manual)
                if self.DEBUG:
                    print("JSON íŒŒì‹± ì„±ê³µ")

                if 'result' in manual_data and len(manual_data['result']) > 0:
                    print(manual_data['result'][0].get('SJ', ''))
                    print()
                    print("ENGL_CMGG\t KOREAN_CMGG\t\t\t DETAIL_CN")
                    print("-" * 80)

                    for item in manual_data['result']:
                        engl = item.get('ENGL_CMGG', '')
                        korean = item.get('KOREAN_CMGG', '')
                        detail = item.get('DETAIL_CN', '').replace('\\n', '\n').replace('\\t', '\t')
                        print(f"{engl:<10}\t {korean:<15}\t|\t {detail}\n")
            except json.JSONDecodeError as e:
                if self.DEBUG:
                    print("JSON íŒŒì‹± ì˜¤ë¥˜:", e)
                else:
                    print("API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜")
        except requests.RequestException as e:
            print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")

    def _merge_parts(self, target_dir: str) -> None:
        """
        Merge split part files in the target directory.

        Finds files matching pattern '*.part*', groups them by prefix,
        and merges them into single files.

        Args:
            target_dir (str): Directory containing part files to merge.
        """
        target_path = Path(target_dir)
        part_files = list(target_path.glob("*.part*"))

        if not part_files:
            return

        # prefixë³„ë¡œ ê·¸ë£¹í™”
        prefixes: Dict[str, List[Tuple[int, Path]]] = {}
        for part_file in part_files:
            match = re.match(r'(.+)\.part(\d+)$', part_file.name)
            if match:
                prefix = match.group(1)
                part_num = int(match.group(2))
                if prefix not in prefixes:
                    prefixes[prefix] = []
                prefixes[prefix].append((part_num, part_file))

        # ê° prefixë³„ë¡œ ë³‘í•©
        for prefix, parts in prefixes.items():
            print(f"Merging {prefix} in {target_dir}")
            parts.sort(key=lambda x: x[0])  # part ë²ˆí˜¸ë¡œ ì •ë ¬

            output_path = target_path / prefix
            with open(output_path, 'wb') as output_file:
                for _, part_file in parts:
                    with open(part_file, 'rb') as input_file:
                        shutil.copyfileobj(input_file, output_file)
            
            # part íŒŒì¼ë“¤ ì‚­ì œ
            for _, part_file in parts:
                part_file.unlink()

    def _merge_parts_all(self, base_path: str = ".") -> None:
        """
        Recursively merge all part files in subdirectories.

        Walks through directory tree from base_path and merges
        any split part files found.

        Args:
            base_path (str, optional): Root directory to start search.
                Defaults to ".".
        """
        if self.DEBUG:
            print("ë³‘í•© ì¤‘ì…ë‹ˆë‹¤...")
        for root, dirs, files in os.walk(base_path):
            part_files = [f for f in files if '.part' in f]
            if part_files:
                self._merge_parts(root)
        if self.DEBUG:
            print("ë³‘í•©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    def download_dataset(
        self,
        apikey: str,
        datasetkey: int,
        filekeys: str = "all",
        overwrite: bool = False
    ) -> List[str]:
        """
        Download dataset from AI Hub.

        Args:
            apikey (str): AI Hub API key for authentication.
            datasetkey (int): Dataset identifier number.
            filekeys (str, optional): File keys to download. Use 'all' for
                all files or comma-separated keys like '66065,66083'.
                Defaults to "all".
            overwrite (bool, optional): If True, overwrite existing files.
                Defaults to False.

        Returns:
            List[str]: List of extracted file paths.

        Raises:
            requests.RequestException: If download request fails.
        """
        def _parse_size(size_str: str) -> float:
            """
            Convert size string to bytes.

            Args:
                size_str (str): Size string like '92 GB', '8 MB', etc.

            Returns:
                float: Size in bytes.
            """
            size_str = size_str.strip().upper()
            if 'GB' in size_str:
                return float(size_str.replace('GB', '').strip()) * 1024**3
            elif 'MB' in size_str:
                return float(size_str.replace('MB', '').strip()) * 1024**2
            elif 'KB' in size_str:
                return float(size_str.replace('KB', '').strip()) * 1024
            elif 'B' in size_str:
                return float(size_str.replace('B', '').strip())
            return 0.0

        download_path = Path(self.download_dir)
        download_tar_path = download_path / "download.tar"

        download_list = self.list_info(
            datasetkey=datasetkey,
            filekeys=filekeys,
            print_out=False
        )

        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì€ ì œì™¸
        keys_to_download: List[str] = []
        for key, info in download_list.items():
            extracted_file_path = os.path.join(self.download_dir, info.path)
            if not overwrite and os.path.exists(extracted_file_path):
                print(f"íŒŒì¼ ë°œê²¬: {extracted_file_path}")
                if self.DEBUG:
                    print("ë‹¤ìš´ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
                continue

            # ì••ì¶• í•´ì§€ í•˜ê³  ìš©ëŸ‰ ì´ìŠˆë¡œ ì¸í•˜ì—¬ zipíŒŒì¼ì€ ì‚­ì œ ë˜ì—ˆë‹¤.
            if not overwrite and os.path.exists(extracted_file_path + ".unzip"):
                print(f"íŒŒì¼ ë°œê²¬ unzip: {extracted_file_path}.unzip")
                if self.DEBUG:
                    print("ë‹¤ìš´ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
                continue

            keys_to_download.append(str(key))

        # ë‹¤ìš´ë¡œë“œí•  filekeysê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not keys_to_download:
            print("ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            extracted_files: List[str] = []
            for key, info in download_list.items():
                file_path = os.path.join(self.download_dir, info.path)
                if os.path.exists(file_path):
                    extracted_files.append(file_path)
            print("ë‹¤ìš´ë¡œë“œ íŒŒì¼ ëª©ë¡:", extracted_files)
            return extracted_files

        # í—¤ë”ì™€ íŒŒë¼ë¯¸í„° ê¸°ë³¸ ì„¤ì •
        headers = {"apikey": apikey}
        params = {"fileSn": ",".join(keys_to_download)}

        mode = "wb"
        existing_size = 0
        response_head = requests.head(
            f"{self.BASE_DOWNLOAD_URL}/{datasetkey}.do",
            headers=headers,
            params=params
        )
        if "content-length" in response_head.headers:
            total_size = int(response_head.headers.get('content-length', 0))
        else:
            total_size = 0
            if self.DEBUG:
                print("content-length í—¤ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í¬ê¸° ì•Œ ìˆ˜ ì—†ìŒ.")
                print("HEAD ì‘ë‹µ í—¤ë”:", response_head.headers)

        if total_size == 0:
            total_size = int(sum(
                _parse_size(info.size) for info in download_list.values()
            ))
            if self.DEBUG:
                print(f"download_list ê¸°ë°˜ ì¶”ì • total_size: "
                      f"{total_size / (1024**3):.2f} GB")

        # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ
        if self.DEBUG:
            print("ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

        os.makedirs(download_path, exist_ok=True)
        response = requests.get(
            f"{self.BASE_DOWNLOAD_URL}/{datasetkey}.do",
            headers=headers,
            params=params,
            stream=True
        )

        if response.status_code in [200, 206]:

            with open(download_tar_path, mode) as f, tqdm(
                total=total_size,
                unit='B',
                unit_scale=True,
                desc="Downloading",
                mininterval=3.0,  # 3ì´ˆë§ˆë‹¤ ê°±ì‹ 
                initial=(existing_size if mode == "ab" else 0)
            ) as pbar:
                update_count = 1000
                downloaded = existing_size if mode == "ab" else 0
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    #f.flush()
                    downloaded += len(chunk)
                    pbar.update(len(chunk))
                    if update_count <= 0:
                        pbar.set_postfix_str(
                            f"{downloaded / (1024**2):.2f}MB / "
                            f"{total_size / (1024**2):.2f}MB"
                        )
                        update_count = 1000
                    update_count -= 1
                f.flush()

            if self.DEBUG:
                print("ì••ì¶• í•´ì œ ì¤‘...")
            with tarfile.open(download_tar_path, "r") as tar:
                tar.extractall(path=download_path)
            self._merge_parts_all(str(download_path))
            download_tar_path.unlink()

            print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        else:
            print(f"Download failed with HTTP status {response.status_code}.")
            print("Error msg:")
            print(response.text)
            if download_tar_path.exists():
                download_tar_path.unlink()

        extracted_files: List[str] = []
        for key, info in download_list.items():
            file_path = os.path.join(self.download_dir, info.path)
            if os.path.exists(file_path):
                extracted_files.append(file_path)
        print("ë‹¤ìš´ë¡œë“œ íŒŒì¼ ëª©ë¡:", extracted_files)
        return extracted_files

    def list_info(
        self,
        datasetkey: Optional[int] = None,
        filekeys: str = "all",
        print_out: bool = True
    ) -> Dict[int, Any]:
        """
        Retrieve and display dataset file information.

        Args:
            datasetkey (Optional[int], optional): Dataset key to query.
                Defaults to None.
            filekeys (str, optional): File keys to filter. Use 'all' for all files
                or comma-separated keys. Defaults to "all".
            print_out (bool, optional): If True, print file information table.
                Defaults to True.

        Returns:
            Dict[int, Any]: Dictionary mapping filekey to FileInfo objects.
        """
        resjson = self.json_info(datasetkey=datasetkey)

        # íŒŒì¼ ì •ë³´ë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬
        file_info_dict: Dict[int, Dict[str, Any]] = {}

        def extract_files(structure: List[Dict[str, Any]]) -> None:
            """
            Recursively extract file information from structure.

            Args:
                structure (List[Dict[str, Any]]): Nested file/directory structure.
            """
            for item in structure:
                if item["type"] == "file" and "filekey" in item:
                    filekey = int(item["filekey"])
                    file_info_dict[filekey] = {
                        "filekey": item["filekey"],
                        "filename": item["name"],
                        "size": item["size"],
                        "path": item["path"],
                        "deep": item["deep"]
                    }
                elif item["type"] == "directory" and "children" in item:
                    extract_files(item["children"])

        # JSON êµ¬ì¡°ì—ì„œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        extract_files(resjson["structure"])

        # filekeys ì²˜ë¦¬
        filtered_files: Dict[int, Dict[str, Any]]
        if filekeys == "all":
            filtered_files = file_info_dict
        else:
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ filekeys íŒŒì‹±
            requested_keys: List[int] = []
            for key in filekeys.split(','):
                try:
                    requested_keys.append(int(key.strip()))
                except ValueError:
                    continue

            # ìš”ì²­ëœ filekeyë§Œ í•„í„°ë§
            filtered_files = {
                k: v for k, v in file_info_dict.items()
                if k in requested_keys
            }

        # ì¶œë ¥
        if print_out:
            print(f"Dataset: {datasetkey}")
            print("=" * 80)
            print(f"{'FileKey':<8} {'Filename':<30} {'Size':<10} {'Path'}")
            print("-" * 80)

            for filekey, info in sorted(filtered_files.items()):
                print(f"{info['filekey']:<8} {info['filename']:<30} "
                      f"{info['size']:<10} {info['path']}")

            print(f"\nì´ {len(filtered_files)}ê°œ íŒŒì¼")

        # ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (FileInfo ê°ì²´ í˜•íƒœë¡œ)
        class FileInfo:
            """File information container."""

            def __init__(
                self,
                filekey: str,
                filename: str,
                size: str,
                path: str,
                deep: int
            ) -> None:
                """
                Initialize FileInfo.

                Args:
                    filekey (str): File key identifier.
                    filename (str): Name of the file.
                    size (str): File size string.
                    path (str): File path.
                    deep (int): Directory depth level.
                """
                self.filekey = filekey
                self.filename = filename
                self.size = size
                self.path = path
                self.deep = deep

            def __str__(self) -> str:
                return (f"FileInfo(filekey={self.filekey}, "
                        f"filename='{self.filename}', size='{self.size}', "
                        f"path='{self.path}', deep={self.deep})")

            def __repr__(self) -> str:
                return self.__str__()

        result_dict: Dict[int, FileInfo] = {}
        for filekey, info in filtered_files.items():
            result_dict[filekey] = FileInfo(
                filekey=info["filekey"],
                filename=info["filename"],
                size=info["size"],
                path=info["path"],
                deep=info["deep"]
            )

        return result_dict

    def dataset_info(
        self,
        datasetkey: Optional[int] = None,
        datasetname: Optional[str] = None
    ) -> None:
        """
        Fetch and display dataset list or file tree structure.

        Args:
            datasetkey (Optional[int], optional): Dataset key for file tree.
                Defaults to None.
            datasetname (Optional[str], optional): Dataset name (unused).
                Defaults to None.

        Raises:
            requests.RequestException: If API request fails.
        """
        if datasetkey:
            filetree_url = f"{self.BASE_FILETREE_URL}/{datasetkey}.do"
            print("Fetching file tree structure...")
            try:
                response = requests.get(filetree_url)
                # ì¸ì½”ë”© ìë™ ê°ì§€
                response.encoding = response.apparent_encoding
                print(response.text)
            except requests.RequestException as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
        else:
            print("Fetching dataset information...")
            try:
                response = requests.get(self.DATASET_URL)
                response.encoding = 'utf-8'
                print(response.text)
            except requests.RequestException as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")

    def dataset_search(
        self,
        datasetname: Optional[str] = None,
        tree: bool = False
    ) -> None:
        """
        Search datasets by name and optionally show file tree.

        Args:
            datasetname (Optional[str], optional): Dataset name to search for
                (partial match). Defaults to None.
            tree (bool, optional): If True, also display file tree for matches.
                Defaults to False.

        Raises:
            requests.RequestException: If API request fails.
        """
        print("Fetching dataset information...")
        try:
            response = requests.get(self.DATASET_URL)
            response.encoding = 'utf-8'
            text = response.text
            if datasetname:
                # datasetnameì´ í¬í•¨ëœ ë¶€ë¶„ë§Œ ì¶œë ¥
                lines = text.splitlines()
                for line in lines:
                    if datasetname in line:
                        # 576, ê²½êµ¬ì•½ì œ ì´ë¯¸ì§€ ë°ì´í„°
                        num, name = line.split(',', 1)
                        # í•´ë‹¹ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ
                        if tree:
                            self.dataset_info(datasetkey=int(num.strip()))
                        else:
                            print(line)
            else:
                print(text)
        except requests.RequestException as e:
            print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")

    def _get_depth_from_star_count(
        self,
        star_count: int,
        depth_mapping: List[int]
    ) -> int:
        """
        Convert star_count to depth level.

        Maintains a sorted mapping of star counts to depth levels.

        Args:
            star_count (int): Number of stars/indentation level.
            depth_mapping (List[int]): List of known star counts.

        Returns:
            int: Depth level (index in sorted mapping).
        """
        if star_count not in depth_mapping:
            # ìƒˆë¡œìš´ star_count ê°’ì´ë©´ ë°°ì—´ì— ì¶”ê°€
            depth_mapping.append(star_count)
            # ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
            depth_mapping.sort()
        
        # ë°°ì—´ì—ì„œì˜ ì¸ë±ìŠ¤ê°€ ê¹Šì´
        return depth_mapping.index(star_count)

    def _json_line(self, line, json_obj, depth_mapping, path_stack, weight=0, deep=0):
        """íŒŒì¼ íŠ¸ë¦¬ì˜ í•œ ì¤„ì„ JSON êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±í•˜ì—¬ ì¶”ê°€"""
        # íŠ¸ë¦¬ êµ¬ì¡° ê¸°í˜¸ë¥¼ ëª¨ë‘ *ë¡œ ë³€ê²½
        line = line.replace("â”œâ”€", "â””â”€")
        line = line.replace("â”‚ ", "â””â”€")
        while "    â””â”€" in line:
            line = line.replace("    â””â”€", "â””â”€â””â”€")
        while " â””â”€" in line:
            line = line.replace(" â””â”€", "â””â”€")
        
        while "â””â”€" in line:
            line = line.replace("â””â”€", "*")
        
        # ì•ë¶€ë¶„ì˜ * ê°œìˆ˜ì™€ ë¬¸ìì—´ ì¶”ì¶œ
        star_count = 0
        for char in line:
            if char == '*':
                star_count += 1
            else:
                break
        clean_str = line.replace('*', '').strip()
        
        # star_countë¥¼ deepë¡œ ë™ì  ë³€í™˜
        deep = self._get_depth_from_star_count(star_count, depth_mapping)
        
        has_pipe = "|" in line
        
        # íŒŒì¼/í´ë” ì •ë³´ ì¶”ì¶œ
        if has_pipe:
            parts = clean_str.split('|')
            if len(parts) >= 3:
                filename = parts[0].strip()
                size = parts[1].strip()
                filekey = parts[2].strip()
                item_type = "file"
            else:
                filename = clean_str
                size = ""
                filekey = ""
                item_type = "directory"
        else:
            filename = clean_str
            size = ""
            filekey = ""
            item_type = "directory"
        
        # path_stack ì¡°ì • (í˜„ì¬ ê¹Šì´ì— ë§ê²Œ)
        while len(path_stack) > deep:
            path_stack.pop()
        
        # í˜„ì¬ ì•„ì´í…œ ì •ë³´
        current_item = {
            "name": filename,
            "type": item_type,
            "deep": deep,
            "weight": star_count,
            "path": str(Path(*path_stack, filename)).replace(' ', '_')  # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        }
        
        if item_type == "file":
            current_item["size"] = size
            current_item["filekey"] = filekey
        else:
            current_item["children"] = []
        
        # JSON êµ¬ì¡°ì— ì¶”ê°€ (ë°°ì—´ êµ¬ì¡°)
        current_array = json_obj
        for path_name in path_stack:
            # í•´ë‹¹ ì´ë¦„ì˜ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ì„œ ê·¸ children ë°°ì—´ë¡œ ì´ë™
            found = None
            for item in current_array:
                if item["name"] == path_name and item["type"] == "directory":
                    found = item
                    break
            if found:
                current_array = found["children"]
        
        # í˜„ì¬ ë°°ì—´ì— ì•„ì´í…œ ì¶”ê°€
        current_array.append(current_item)
        
        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° path_stackì— ì¶”ê°€
        if item_type == "directory":
            path_stack.append(filename)
        
        # if self.DEBUG:
        #     print(f"[deep={deep}] [weight={star_count}] {item_type[0].upper()} {filename}" + 
        #         (f" , {size} , {filekey}" if item_type == "file" else " , , "))
        
        return current_item

    def json_info(self, datasetkey=None):
        """ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŒŒì¼ íŠ¸ë¦¬ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜"""
        filetree_url = f"{self.BASE_FILETREE_URL}/{datasetkey}.do"        
        response = requests.get(filetree_url)
        response.encoding = response.apparent_encoding
        text = response.text
        
        # JSON êµ¬ì¡°ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        result = {
            "datasetkey": datasetkey,
            "structure": []  # ë°°ì—´ë¡œ ë³€ê²½
        }
        
        lines = text.splitlines()
        
        is_notify = True
        json_obj = []  # ë£¨íŠ¸ ë°°ì—´
        depth_mapping = []  # ê° íŒŒì‹± ì„¸ì…˜ë§ˆë‹¤ ìƒˆë¡œìš´ depth_mapping
        path_stack = []     # í˜„ì¬ ê²½ë¡œë¥¼ ì¶”ì í•˜ëŠ” ìŠ¤íƒ

        # if self.DEBUG:
        #     test_count = 10

        for line in lines:
            if not line.strip() or 'ê³µì§€ì‚¬í•­' in line or '=' in line:
                is_notify = False
                continue
            if is_notify:
                continue

            self._json_line(line, json_obj, depth_mapping, path_stack, weight=0, deep=0)

            # if self.DEBUG:
            #     test_count -= 1
            #     if test_count <= 0:
            #         break
        
        result["structure"] = json_obj

        return result


def get_tqdm_kwargs() -> Dict[str, Any]:
    """
    Get safe tqdm configuration to prevent widget errors.

    Returns:
        Dict[str, Any]: Configuration dictionary for tqdm.
    """
    return {
        'disable': False,
        'leave': True,
        'file': sys.stdout,
        'ascii': True,  # ASCII ë¬¸ìë§Œ ì‚¬ìš©
        'dynamic_ncols': False,
    }


def drive_root() -> str:
    """
    Get Google Drive root path.

    Returns:
        str: Path to Google Drive root directory.
            - Colab: '/content/drive/MyDrive'
            - Windows: 'D:\\GoogleDrive'
    """
    root_path = os.path.join("D:\\", "GoogleDrive")
    if IS_COLAB:
        root_path = os.path.join("/content/drive/MyDrive")
    return root_path


def get_path_modeling(add_path: Optional[str] = None) -> str:
    """
    Get modeling directory path.

    Args:
        add_path (Optional[str], optional): Additional path to append.
            Defaults to None.

    Returns:
        str: Full path to modeling directory.
    """
    modeling_path = "modeling"
    path = os.path.join(drive_root(), modeling_path)
    if add_path is not None:
        path = os.path.join(path, add_path)
    return path


def get_path_modeling_release(add_path: Optional[str] = None) -> str:
    """
    Get modeling release directory path.

    Args:
        add_path (Optional[str], optional): Additional path to append.
            Defaults to None.

    Returns:
        str: Full path to modeling_release directory.
    """
    modeling_path = "modeling_release"
    path = os.path.join(drive_root(), modeling_path)
    if add_path is not None:
        path = os.path.join(path, add_path)
    return path


def get_path_temp(add_path: Optional[str] = None) -> str:
    """
    Get temporary directory path.

    Args:
        add_path (Optional[str], optional): Additional path to append.
            Defaults to None.

    Returns:
        str: Full path to temporary directory.
            - Colab: '/content/temp'
            - Windows: 'D:\\temp' (or current drive)
    """
    if IS_COLAB:
        temp_path = r"/content/temp"
    else:
        drive = os.path.splitdrive(os.getcwd())[0]  # ex: 'D:'
        temp_path = os.path.join(drive + os.sep, 'temp')
    if add_path is not None:
        temp_path = os.path.join(temp_path, add_path)
    return temp_path


def download_gdrive_file(url: str, output_path: str, ignore: bool = True) -> None:
    """
    Download file from Google Drive.

    Args:
        url (str): Google Drive share link.
        output_path (str): Output file path.
        ignore (bool, optional): If True, delete existing file before download.
            If False, skip download if file exists. Defaults to True.

    Raises:
        ImportError: If gdown module is not installed.
        ValueError: If Google Drive file ID cannot be found in URL.
    """
    try:
        import gdown
    except ImportError:
        raise ImportError("gdown ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤. 'pip install gdown'ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

    # ê³µìœ  ë§í¬ì—ì„œ íŒŒì¼ ID ì¶”ì¶œ
    if os.path.exists(output_path):
        if ignore:
            os.remove(output_path)
        else:
            return

    file_id_match = re.search(r'/d/([a-zA-Z0-9_-]+)', url)
    if not file_id_match:
        raise ValueError("Google Drive íŒŒì¼ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    file_id = file_id_match.group(1)

    gdown.download(f"https://drive.google.com/uc?id={file_id}", output_path, quiet=False)


def download_http(url: str, output_path: str, ignore: bool = True) -> str:
    """
    Download file via HTTP with progress bar.

    Args:
        url (str): URL of file to download.
        output_path (str): Output file path.
        ignore (bool, optional): If True, delete existing file before download.
            If False, skip download if file exists. Defaults to True.

    Returns:
        str: Path to downloaded file.

    Raises:
        requests.RequestException: If download request fails.
    """
    if os.path.exists(output_path):
        if ignore:
            os.remove(output_path)
        else:
            print(f"ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {output_path}")
            return output_path

    # í´ë” ìƒì„±
    output_dir = os.path.dirname(output_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    response = requests.get(url, stream=True)
    total = int(response.headers.get('content-length', 0))
    with open(output_path, 'wb') as file, tqdm(
        desc=f"Downloading {os.path.basename(output_path)}",
        total=total,
        unit='B',
        unit_scale=True,
        unit_divisor=1024,
        ascii=True
    ) as bar:
        for data in response.iter_content(chunk_size=1024):
            size = file.write(data)
            bar.update(size)
    print(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
    return output_path


def print_dir_tree(
    root: str,
    indent: str = "",
    max_file_list: Optional[int] = None,
    max_dir_list: Optional[int] = None
) -> None:
    """
    Print directory tree structure.

    Args:
        root (str): Root directory path.
        indent (str, optional): Indentation string. Defaults to "".
        max_file_list (Optional[int], optional): Maximum number of files to
            display per directory. None means show all. Defaults to None.
        max_dir_list (Optional[int], optional): Maximum number of subdirectories
            to display per directory. None means show all. Defaults to None.
    """
    try:
        entries = sorted(os.listdir(root))
    except Exception as e:
        print(indent + f"[Error] {e}")
        return

    # ë””ë ‰í† ë¦¬ / íŒŒì¼ ë¶„ë¦¬
    dirs = [e for e in entries if os.path.isdir(os.path.join(root, e))]
    files = [e for e in entries if not os.path.isdir(os.path.join(root, e))]

    total_dirs = len(dirs)
    total_files = len(files)

    display_dirs = dirs if max_dir_list is None else dirs[:max_dir_list]
    display_files = files if max_file_list is None else files[:max_file_list]

    allowed_dirs = set(display_dirs)
    allowed_files = set(display_files)

    has_more_dirs = (max_dir_list is not None) and (total_dirs > max_dir_list)
    has_more_files = (max_file_list is not None) and (total_files > max_file_list)

    for entry in entries:
        path = os.path.join(root, entry)
        if os.path.isdir(path):
            if entry not in allowed_dirs:
                continue
            print(indent + "|-- " + entry)
            # ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ê°œìˆ˜ ì¶œë ¥
            try:
                file_count = len([f for f in os.listdir(path)])
            except Exception:
                file_count = 0
            print(indent + "   " + f"[ë°ì´í„°íŒŒì¼: {file_count}ê°œ]")
            # ì¬ê·€ í˜¸ì¶œ ì‹œ ë™ì¼í•œ ì œí•œ ì „ë‹¬
            print_dir_tree(
                root=path,
                indent=indent + "   ",
                max_file_list=max_file_list,
                max_dir_list=max_dir_list
            )
        else:
            if entry not in allowed_files:
                continue
            print(indent + "|-- " + entry)

    # ìƒëµëœ í•­ëª©ì´ ìˆìœ¼ë©´ í‘œì‹œ
    if has_more_dirs:
        print(indent + "   " + "... dirs")
    if has_more_files:
        print(indent + "   " + "... files")
            

def print_json_tree(data, indent="", max_depth=4, _depth=0, list_count=1, print_value=True, limit_value_text=100):
    """
    JSON ê°ì²´ë¥¼ ì§€ì •í•œ ë‹¨ê³„(max_depth)ê¹Œì§€ íŠ¸ë¦¬ í˜•íƒœë¡œ ì¶œë ¥
    - list íƒ€ì…ì€ ì•/ë’¤ list_countê°œìˆ˜ ë§Œí¼ ì¶œë ¥í•˜ê³  ì¤‘ê°„ì€ "..."ë¡œ ìƒëµ ì²˜ë¦¬
    - í•˜ìœ„ ë…¸ë“œê°€ ê°’ì¼ ê²½ìš° key(type) í˜•íƒœë¡œ ì¶œë ¥
    - print_value=Trueì¼ ë•Œ key(type): ê°’ í˜•íƒœë¡œ ì¶œë ¥
    """
    if _depth > max_depth:
        return

    if isinstance(data, dict):
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                print(f"{indent}|-- {key}")
                print_json_tree(value, indent + "    ", max_depth, _depth + 1, list_count, print_value, limit_value_text)
            else:
                if print_value:
                    vstr = str(value)
                    short = vstr if len(vstr) < limit_value_text else f"{vstr[:30]}..."
                    print(f"{indent}|-- {key}({type(value).__name__}): {short}")
                else:
                    print(f"{indent}|-- {key}({type(value).__name__})")

    elif isinstance(data, list):
        n = int(list_count) if list_count is not None else 0
        L = len(data)

        if L == 0:
            print(f"{indent}|-- [list] (0 items)")
            return

        # ë¦¬ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸¸ë©´ ì•/ë’¤ nê°œë§Œ ë³´ì—¬ì£¼ê³  ì¤‘ê°„ ìƒëµ
        if n > 0 and L > 2 * n:
            print(f"{indent}|-- [list] ({L} items)")
            # ì•ìª½ nê°œ
            for i in range(0, n):
                item = data[i]
                if isinstance(item, (dict, list)):
                    print(f"{indent}    |-- [{i}]")
                    print_json_tree(item, indent + "        ", max_depth, _depth + 1, list_count, print_value, limit_value_text)
                else:
                    if print_value:
                        vstr = str(item)
                        short = vstr if len(vstr) < limit_value_text else f"{vstr[:30]}..."
                        print(f"{indent}    |-- [{i}]({type(item).__name__}): {short}")
                    else:
                        print(f"{indent}    |-- [{i}]({type(item).__name__})")

            # ìƒëµ í‘œì‹œ
            omitted = L - 2 * n
            print(f"{indent}    |-- ... ({omitted} items omitted)")

            # ë’¤ìª½ nê°œ
            for j in range(L - n, L):
                item = data[j]
                if isinstance(item, (dict, list)):
                    print(f"{indent}    |-- [{j}]")
                    print_json_tree(item, indent + "        ", max_depth, _depth + 1, list_count, print_value, limit_value_text)
                else:
                    if print_value:
                        vstr = str(item)
                        short = vstr if len(vstr) < limit_value_text else f"{vstr[:30]}..."
                        print(f"{indent}    |-- [{j}]({type(item).__name__}): {short}")
                    else:
                        print(f"{indent}    |-- [{j}]({type(item).__name__})")

        else:
            # ì „ì²´ ì¶œë ¥ (list_countê°€ 0 ì´ê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì§§ì€ ê²½ìš°)
            print(f"{indent}|-- [list] ({L} items)")
            for i, item in enumerate(data):
                if isinstance(item, (dict, list)):
                    print(f"{indent}    |-- [{i}]")
                    print_json_tree(item, indent + "        ", max_depth, _depth + 1, list_count, print_value, limit_value_text)
                else:
                    if print_value:
                        vstr = str(item)
                        short = vstr if len(vstr) < limit_value_text else f"{vstr[:30]}..."
                        print(f"{indent}    |-- [{i}]({type(item).__name__}): {short}")
                    else:
                        print(f"{indent}    |-- [{i}]({type(item).__name__})")
    else:
        if print_value:
            vstr = str(data)
            short = vstr if len(vstr) < limit_value_text else f"{vstr[:30]}..."
            print(f"{indent}{type(data).__name__}: {short}")
        else:
            print(f"{indent}{type(data).__name__}")

def print_dic_tree(dic_data, indent="", max_depth=3, _depth=0, list_count=1, print_value=True, limit_value_text=100):
    """
    PyTorch tensor/ë”•ì…”ë„ˆë¦¬/ë¦¬ìŠ¤íŠ¸ë¥¼ git tree ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥
    - max_depth: ì¶œë ¥í•  ìµœëŒ€ ê¹Šì´
    - list_count: ë¦¬ìŠ¤íŠ¸ ì•/ë’¤ë¡œ ì¶œë ¥í•  í•­ëª© ìˆ˜ (ì¤‘ê°„ ìƒëµ)
    - print_value: Trueì´ë©´ ê°’ë„ ì¶œë ¥, Falseì´ë©´ íƒ€ì…ë§Œ ì¶œë ¥
    - limit_value_text: ê°’ ì¶œë ¥ ì‹œ ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
    """

    if _depth > max_depth:
        return
    
    if isinstance(dic_data, dict):
        for key, value in dic_data.items():
            if isinstance(value, (dict, list, tuple)):
                print(f"{indent}â”œâ”€ {key} [{type(value).__name__}]")
                print_dic_tree(value, indent + "â”‚  ", max_depth, _depth + 1, list_count, print_value, limit_value_text)
            elif torch.is_tensor(value):
                shape = tuple(value.shape)
                dtype = str(value.dtype)
                if print_value:
                    preview = str(value)
                    preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
                    print(f"{indent}â”œâ”€ {key} [Tensor] shape={shape} dtype={dtype}")
                    print(f"{indent}â”‚  â””â”€ {preview_str}")
                else:
                    print(f"{indent}â”œâ”€ {key} [Tensor] shape={shape} dtype={dtype}")
            elif isinstance(value, np.ndarray):
                shape = value.shape
                dtype = value.dtype
                if print_value:
                    preview = str(value)
                    preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
                    print(f"{indent}â”œâ”€ {key} [ndarray] shape={shape} dtype={dtype}")
                    print(f"{indent}â”‚  â””â”€ {preview_str}")
                else:
                    print(f"{indent}â”œâ”€ {key} [ndarray] shape={shape} dtype={dtype}")
            else:
                if print_value:
                    val_str = str(value)
                    short = val_str[:limit_value_text] + ("..." if len(val_str) > limit_value_text else "")
                    print(f"{indent}â”œâ”€ {key} [{type(value).__name__}]: {short}")
                else:
                    print(f"{indent}â”œâ”€ {key} [{type(value).__name__}]")
    
    elif isinstance(dic_data, (list, tuple)):
        n = int(list_count) if list_count is not None else 0
        L = len(dic_data)
        
        if L == 0:
            print(f"{indent}â””â”€ [{type(dic_data).__name__}] (0 items)")
            return
        
        # ë¦¬ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸¸ë©´ ì•/ë’¤ nê°œë§Œ ë³´ì—¬ì£¼ê³  ì¤‘ê°„ ìƒëµ
        if n > 0 and L > 2 * n:
            # ì•ìª½ nê°œ
            for i in range(0, n):
                item = dic_data[i]
                if isinstance(item, (dict, list, tuple)):
                    print(f"{indent}â”œâ”€ [{i}] [{type(item).__name__}]")
                    print_dic_tree(item, indent + "â”‚  ", max_depth, _depth + 1, list_count, print_value, limit_value_text)
                elif torch.is_tensor(item):
                    shape = tuple(item.shape)
                    dtype = str(item.dtype)
                    if print_value:
                        preview = str(item)
                        preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{i}] [Tensor] shape={shape} dtype={dtype}: {preview_str}")
                    else:
                        print(f"{indent}â”œâ”€ [{i}] [Tensor] shape={shape} dtype={dtype}")
                elif isinstance(item, np.ndarray):
                    shape = item.shape
                    dtype = item.dtype
                    if print_value:
                        preview = str(item)
                        preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{i}] [ndarray] shape={shape} dtype={dtype}: {preview_str}")
                    else:
                        print(f"{indent}â”œâ”€ [{i}] [ndarray] shape={shape} dtype={dtype}")
                else:
                    if print_value:
                        val_str = str(item)
                        short = val_str[:limit_value_text] + ("..." if len(val_str) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{i}] [{type(item).__name__}]: {short}")
                    else:
                        print(f"{indent}â”œâ”€ [{i}] [{type(item).__name__}]")
            
            # ìƒëµ í‘œì‹œ
            omitted = L - 2 * n
            print(f"{indent}â”œâ”€ ... ({omitted} items omitted)")
            
            # ë’¤ìª½ nê°œ
            for j in range(L - n, L):
                item = dic_data[j]
                if isinstance(item, (dict, list, tuple)):
                    print(f"{indent}â”œâ”€ [{j}] [{type(item).__name__}]")
                    print_dic_tree(item, indent + "â”‚  ", max_depth, _depth + 1, list_count, print_value, limit_value_text)
                elif torch.is_tensor(item):
                    shape = tuple(item.shape)
                    dtype = str(item.dtype)
                    if print_value:
                        preview = str(item)
                        preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{j}] [Tensor] shape={shape} dtype={dtype}: {preview_str}")
                    else:
                        print(f"{indent}â”œâ”€ [{j}] [Tensor] shape={shape} dtype={dtype}")
                elif isinstance(item, np.ndarray):
                    shape = item.shape
                    dtype = item.dtype
                    if print_value:
                        preview = str(item)
                        preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{j}] [ndarray] shape={shape} dtype={dtype}: {preview_str}")
                    else:
                        print(f"{indent}â”œâ”€ [{j}] [ndarray] shape={shape} dtype={dtype}")
                else:
                    if print_value:
                        val_str = str(item)
                        short = val_str[:limit_value_text] + ("..." if len(val_str) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{j}] [{type(item).__name__}]: {short}")
                    else:
                        print(f"{indent}â”œâ”€ [{j}] [{type(item).__name__}]")
        else:
            # ì „ì²´ ì¶œë ¥
            for i, item in enumerate(dic_data):
                if isinstance(item, (dict, list, tuple)):
                    print(f"{indent}â”œâ”€ [{i}] [{type(item).__name__}]")
                    print_dic_tree(item, indent + "â”‚  ", max_depth, _depth + 1, list_count, print_value, limit_value_text)
                elif torch.is_tensor(item):
                    shape = tuple(item.shape)
                    dtype = str(item.dtype)
                    if print_value:
                        preview = str(item)
                        preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{i}] [Tensor] shape={shape} dtype={dtype}: {preview_str}")
                    else:
                        print(f"{indent}â”œâ”€ [{i}] [Tensor] shape={shape} dtype={dtype}")
                elif isinstance(item, np.ndarray):
                    shape = item.shape
                    dtype = item.dtype
                    if print_value:
                        preview = str(item)
                        preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{i}] [ndarray] shape={shape} dtype={dtype}: {preview_str}")
                    else:
                        print(f"{indent}â”œâ”€ [{i}] [ndarray] shape={shape} dtype={dtype}")
                else:
                    if print_value:
                        val_str = str(item)
                        short = val_str[:limit_value_text] + ("..." if len(val_str) > limit_value_text else "")
                        print(f"{indent}â”œâ”€ [{i}] [{type(item).__name__}]: {short}")
                    else:
                        print(f"{indent}â”œâ”€ [{i}] [{type(item).__name__}]")
    
    elif torch.is_tensor(dic_data):
        shape = tuple(dic_data.shape)
        dtype = str(dic_data.dtype)
        if print_value:
            preview = str(dic_data)
            preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
            print(f"{indent}â””â”€ Tensor shape={shape} dtype={dtype}")
            print(f"{indent}   {preview_str}")
        else:
            print(f"{indent}â””â”€ Tensor shape={shape} dtype={dtype}")
    
    elif isinstance(dic_data, np.ndarray):
        shape = dic_data.shape
        dtype = dic_data.dtype
        if print_value:
            preview = str(dic_data)
            preview_str = preview[:limit_value_text] + ("..." if len(preview) > limit_value_text else "")
            print(f"{indent}â””â”€ ndarray shape={shape} dtype={dtype}")
            print(f"{indent}   {preview_str}")
        else:
            print(f"{indent}â””â”€ ndarray shape={shape} dtype={dtype}")
    
    else:
        if print_value:
            val_str = str(dic_data)
            short = val_str[:limit_value_text] + ("..." if len(val_str) > limit_value_text else "")
            print(f"{indent}â””â”€ {type(dic_data).__name__}: {short}")
        else:
            print(f"{indent}â””â”€ {type(dic_data).__name__}")

################################################################################################################

def save_model_dict(model, path, pth_name, kwargs=None):
    """
    ëª¨ë¸ state_dictì™€ ì¶”ê°€ ì •ë³´ë¥¼ ì €ì¥
    """
    def safe_makedirs(path):
        """ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if os.path.exists(path) and not os.path.isdir(path):
            os.remove(path)  # íŒŒì¼ì´ë©´ ì‚­ì œ
        os.makedirs(path, exist_ok=True)

    # ë””ë ‰í† ë¦¬ ìƒì„±
    safe_makedirs(path)

    # ëª¨ë¸ êµ¬ì¡° ì •ë³´ ì¶”ì¶œ
    model_info = {
        'class_name': model.__class__.__name__,
        'init_args': {},
        'str': str(model),
        'repr': repr(model),
        'modules': [m.__class__.__name__ for m in model.modules()],
    }

    # ìƒì„±ì ì¸ì ìë™ ì¶”ì¶œ(ê°€ëŠ¥í•œ ê²½ìš°)
    if hasattr(model, '__dict__'):
        for key in ['in_ch', 'base_ch', 'num_classes', 'out_ch']:
            if hasattr(model, key):
                model_info['init_args'][key] = getattr(model, key)

    # kwargs ì²˜ë¦¬
    extra_info = {}
    if kwargs is not None:
        if isinstance(kwargs, str):
            extra_info = json.loads(kwargs)
        elif isinstance(kwargs, dict):
            extra_info = kwargs

    model_info.update(extra_info)

    # ì €ì¥í•  dict êµ¬ì„±
    save_dict = {
        'model_state': model.state_dict(),
        'class_name': model.__class__.__name__,
        'model_info': model_info,
    }

    save_path = os.path.join(path, f"{pth_name}.pth")
    torch.save(save_dict, save_path)
    return save_path

def load_model_dict(path, pth_name=None):
    """
    save_model_dictë¡œ ì €ì¥í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ë°˜í™˜ê°’: (model_state, model_info)
    """
    import torch
    load_path = path
    if pth_name is not None:
        load_path = os.path.join(path, f"{pth_name}.pth")
    checkpoint = torch.load(load_path, map_location='cpu', weights_only=False)  # <-- ì—¬ê¸° ì¶”ê°€
    model_state = checkpoint.get('model_state')
    model_info = checkpoint.get('model_info')
    model_info['file_name'] = os.path.basename(load_path)
    return model_state, model_info

################################################################################################################

def search_pth_files(base_path):
    """
    ì…ë ¥ëœ ê²½ë¡œì˜ í•˜ìœ„ í´ë”ë“¤ì—ì„œ pth íŒŒì¼ë“¤ì„ ê²€ìƒ‰
    """
    pth_files = []

    if not os.path.exists(base_path):
        print(f"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
        return pth_files

    print(f"pth íŒŒì¼ ê²€ìƒ‰ ì‹œì‘: {base_path}")

    # í•˜ìœ„ í´ë”ë“¤ì„ ìˆœíšŒí•˜ë©° pth íŒŒì¼ ê²€ìƒ‰
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.pth'):
                pth_path = os.path.join(root, file)
                pth_files.append(pth_path)

    # ê²°ê³¼ ì •ë¦¬ ë° ì¶œë ¥
    if pth_files:
        print(f"\në°œê²¬ëœ pth íŒŒì¼ë“¤ ({len(pth_files)}ê°œ):")
        for i, pth_file in enumerate(pth_files, 1):
            # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ (base_path ê¸°ì¤€)
            rel_path = os.path.relpath(pth_file, base_path)
            print(f" {i:2d}. {rel_path}")
    else:
        print("pth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return pth_files


def save_datasets_as_json(save_datasets, dataset_path):
    """ë°ì´í„°ì…‹ì„ JSON í˜•íƒœë¡œ ì €ì¥"""
    print(f"JSON í˜•íƒœë¡œ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘: {dataset_path}")
    
    # numpy arrayë¥¼ listë¡œ ë³€í™˜
    json_data = {}
    for split in ['train', 'validation', 'test']:
        json_data[split] = {
            'text': save_datasets[split]['text'].tolist() if isinstance(save_datasets[split]['text'], np.ndarray) else list(save_datasets[split]['text']),
            'target': save_datasets[split]['target'].tolist() if isinstance(save_datasets[split]['target'], np.ndarray) else list(save_datasets[split]['target'])
        }
    
    json_data['target_names'] = list(save_datasets['target_names'])
    
    # JSONìœ¼ë¡œ ì €ì¥
    with open(dataset_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"ì €ì¥ ì™„ë£Œ: {dataset_path}")

def load_datasets_from_json(dataset_path):
    """JSONì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
    print(f"JSONì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ: {dataset_path}")
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    # numpy arrayë¡œ ë³€í™˜
    load_datasets = {}
    for split in ['train', 'validation', 'test']:
        load_datasets[split] = {
            'text': np.array(json_data[split]['text']),
            'target': np.array(json_data[split]['target'])
        }
    
    load_datasets['target_names'] = json_data['target_names']
    
    print("ë¡œë“œ ì™„ë£Œ")
    return load_datasets

def create_tqdm(
    iterable: Optional[Any] = None,
    total: Optional[int] = None,
    desc: str = "Progress",
    **kwargs: Any
) -> tqdm:
    """
    Create tqdm progress bar with safe configuration.

    Args:
        iterable (Optional[Any], optional): Iterable object to track.
            Defaults to None.
        total (Optional[int], optional): Total count for manual updates.
            Used when iterable is None. Defaults to None.
        desc (str, optional): Description text. Defaults to "Progress".
        **kwargs (Any): Additional tqdm options.

    Returns:
        tqdm: Configured tqdm progress bar object.
    """
    # ê¸°ë³¸ ì˜µì…˜ ì„¤ì •
    default_kwargs = get_tqdm_kwargs() if 'get_tqdm_kwargs' in globals() else {}
    default_kwargs.update(kwargs)

    if iterable is not None:
        # iterableì´ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
        return tqdm(iterable, desc=desc, **default_kwargs)
    else:
        # manual updateìš© tqdm
        return tqdm(total=total, desc=desc, **default_kwargs)


def reset_tqdm(
    pbar: Optional[tqdm],
    iterable: Optional[Any] = None,
    total: Optional[int] = None,
    desc: Optional[str] = None,
    **kwargs: Any
) -> tqdm:
    """
    Reset existing tqdm object or create new one if None.

    Args:
        pbar (Optional[tqdm]): Existing tqdm object to reset.
        iterable (Optional[Any], optional): New iterable object. Defaults to None.
        total (Optional[int], optional): New total count. Defaults to None.
        desc (Optional[str], optional): New description text. Defaults to None.
        **kwargs (Any): Additional options.

    Returns:
        tqdm: Reset tqdm object.
    """
    if pbar is None:
        return create_tqdm(iterable, total, desc or "Progress", **kwargs)

    # ê¸°ì¡´ pbar ì¬ì„¤ì •
    if total is not None:
        pbar.reset(total=total)
    else:
        pbar.reset()

    if desc is not None:
        pbar.set_description(desc)

    # ë‚´ë¶€ ìƒíƒœ ì´ˆê¸°í™”
    pbar.n = 0
    pbar.last_print_n = 0
    # _timeì€ protectedì´ë¯€ë¡œ ì§ì ‘ ì ‘ê·¼ ëŒ€ì‹  ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©
    pbar.last_print_t = pbar.start_t

    # ì¶”ê°€ ì˜µì…˜ ì ìš©
    default_kwargs = get_tqdm_kwargs() if 'get_tqdm_kwargs' in globals() else {}
    default_kwargs.update(kwargs)

    for key, value in default_kwargs.items():
        if hasattr(pbar, key):
            setattr(pbar, key, value)

    pbar.refresh()
    return pbar


def create_or_reset_tqdm(
    pbar: Optional[tqdm] = None,
    iterable: Optional[Any] = None,
    total: Optional[int] = None,
    desc: str = "Progress",
    **kwargs: Any
) -> tqdm:
    """
    Create or reset tqdm progress bar (unified function).

    Args:
        pbar (Optional[tqdm], optional): Existing tqdm object. If None,
            create new one. Defaults to None.
        iterable (Optional[Any], optional): Iterable object. Defaults to None.
        total (Optional[int], optional): Total count. Defaults to None.
        desc (str, optional): Description text. Defaults to "Progress".
        **kwargs (Any): Additional tqdm options.

    Returns:
        tqdm: New or reset tqdm object.
    """
    if pbar is None:
        # ìƒˆë¡œ ìƒì„±
        return create_tqdm(iterable=iterable, total=total, desc=desc, **kwargs)
    else:
        # ê¸°ì¡´ ê²ƒ ì¬ì„¤ì •
        return reset_tqdm(pbar, iterable=iterable, total=total, desc=desc, **kwargs)

##########################################################################################################

def unzip(zipfile_list, remove_zip=False, skip_root=False, normalize_nfc: bool = True, force_utf8: bool = False):
    def _try_force_utf8(name: str) -> str:
        """CP437â†’UTF-8/CP949 ì¬í•´ì„ (ê°œì„ )"""
        candidates = [
            ('utf-8', 'strict'),      # UTF-8 ZIP
            ('cp949', 'ignore'),      # í•œêµ­ ë ˆê±°ì‹œ
            ('euc_kr', 'ignore'),     # êµ¬í˜• ë¦¬ëˆ…ìŠ¤
            ('cp437', 'replace'),     # DOS í´ë°±
        ]
        for enc, errors in candidates:
            try:
                return name.encode('latin1').decode(enc, errors=errors)
            except (UnicodeDecodeError, UnicodeEncodeError):
                continue
        return name  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

    unzip_paths = []

    for zip_path in zipfile_list:
        if not (os.path.exists(zip_path) and os.path.isfile(zip_path)):
            print(f"ì¡´ì¬í•˜ì§€ ì•Šì€ íŒŒì¼: {zip_path}")
            continue

        extract_dir = zip_path + ".unzip"
        unzip_paths.append(extract_dir)

        if os.path.exists(extract_dir):
            print(f"ì´ë¯¸ ì••ì¶• í•´ì œë¨: {extract_dir}")
            continue

        os.makedirs(extract_dir, exist_ok=True)

        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            members = zip_ref.namelist()

            # ========== skip_root ë¡œì§ (ìˆ˜ì •) ==========
            skip_prefix = ""

            if skip_root and members:
                # ZIP ë‚´ë¶€ ê²½ë¡œëŠ” í•­ìƒ POSIX í˜•ì‹('/')ì´ë¯€ë¡œ '/'ë¡œ ë¶„í• 
                # __MACOSX ê°™ì€ ë©”íƒ€íŒŒì¼ ì œì™¸í•˜ê³  ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ì¶œ
                top_level_dirs = set()
                for m in members:
                    if m.startswith('__MACOSX') or m.startswith('.'):
                        continue
                    parts = m.split('/', 1)  # POSIX êµ¬ë¶„ì ì‚¬ìš©
                    if len(parts) > 0 and parts[0]:
                        top_level_dirs.add(parts[0])

                # ìµœìƒìœ„ ë””ë ‰í† ë¦¬ê°€ 1ê°œë§Œ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ ëŒ€ìƒ
                if len(top_level_dirs) == 1:
                    skip_prefix = list(top_level_dirs)[0] + '/'
                    print(f"ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ìŠ¤í‚µ: {skip_prefix.rstrip('/')}")

            # ========== ì••ì¶• í•´ì œ ==========
            for member_name_orig in tqdm(members, desc=f"ì••ì¶• í•´ì œ ì¤‘: {os.path.basename(zip_path)}", unit="file"):

                # ë©”íƒ€íŒŒì¼ ê±´ë„ˆë›°ê¸°
                if member_name_orig.startswith('__MACOSX') or member_name_orig.startswith('.'):
                    continue

                member_name_to_use = member_name_orig

                # 1. force_utf8 ì˜µì…˜ì´ ì¼œì ¸ìˆìœ¼ë©´ ì¬í•´ì„ ì‹œë„
                if force_utf8:
                    member_name_to_use = _try_force_utf8(member_name_orig)

                # 2. NFD â†’ NFC ë³€í™˜ (ì˜µì…˜)
                if normalize_nfc:
                    member_name_nfc = unicodedata.normalize('NFC', member_name_to_use)
                else:
                    member_name_nfc = member_name_to_use

                # 3. ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ìŠ¤í‚µ (skip_prefixëŠ” ì›ë³¸ ë©¤ë²„ ê¸°ì¤€)
                if skip_prefix and member_name_orig.startswith(skip_prefix):
                    relative_path = member_name_orig[len(skip_prefix):]
                    if not relative_path:
                        continue
                    if force_utf8:
                        relative_path = _try_force_utf8(relative_path)
                    relative_path_nfc = unicodedata.normalize('NFC', relative_path) if normalize_nfc else relative_path
                else:
                    relative_path_nfc = member_name_nfc

                # 4. ì¶”ì¶œ ê²½ë¡œ (OS êµ¬ë¶„ìë¡œ ë³€í™˜)
                target_path = os.path.join(extract_dir, relative_path_nfc.replace('/', os.sep))

                # 5. íŒŒì¼/ë””ë ‰í† ë¦¬ ì¶”ì¶œ
                info = zip_ref.getinfo(member_name_orig)

                if info.is_dir():
                    os.makedirs(target_path, exist_ok=True)
                else:
                    # ìƒìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
                    parent_dir = os.path.dirname(target_path)
                    if parent_dir:
                        os.makedirs(parent_dir, exist_ok=True)

                    # íŒŒì¼ ì¶”ì¶œ
                    with zip_ref.open(member_name_orig) as source, open(target_path, 'wb') as target:
                        target.write(source.read())


            print(f"\nì••ì¶• í•´ì œ ì™„ë£Œ: {extract_dir}")
        # ì›ë³¸ zip ì‚­ì œ
        if remove_zip:
            os.remove(zip_path)
    return unzip_paths


def zip_progress(
    input_path: Union[str, Path],
    zip_path: str,
    compression: Optional[int] = None
) -> Optional[str]:
    """
    Compress file or folder to ZIP with progress bar.

    Maintains relative paths within the compressed file.

    Args:
        input_path (Union[str, Path]): Path to file or folder to compress.
        zip_path (str): Output ZIP file path.
        compression (Optional[int], optional): Compression method.
            Defaults to zipfile.ZIP_DEFLATED.

    Returns:
        Optional[str]: Path to created ZIP file, or None if failed.

    Example:
        >>> zip_progress("my_folder", "archive.zip")
        Zipping: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 100.00file/s]
        'archive.zip'
    """
    input_path = Path(input_path)
    if not input_path.exists():
        print(f"ì••ì¶•í•  ëŒ€ìƒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}")
        return None

    if compression is None:
        compression = zipfile.ZIP_DEFLATED

    # ì••ì¶• ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ìƒì„±
    if input_path.is_dir():
        files = [f for f in input_path.rglob('*') if f.is_file()]
    else:
        files = [input_path]

    if not files:
        print("ì••ì¶•í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ZIP íŒŒì¼ ìƒì„±
    with zipfile.ZipFile(zip_path, 'w', compression) as zf:
        with tqdm(total=len(files), desc="Zipping", unit="file") as pbar:
            for file in files:
                # ì••ì¶• íŒŒì¼ ë‚´ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
                arcname = file.relative_to(
                    input_path.parent if input_path.is_file() else input_path
                )
                zf.write(file, arcname)
                pbar.update(1)

    return zip_path


##########################################################################################################

print('helper_utils.py loaded')
